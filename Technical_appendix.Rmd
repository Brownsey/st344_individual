---
title: "St344 Individual Coursework U1619685"
author: "Stephen Brownsey"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, libraries}
library(tidyverse)
library(lubridate)
library(broom)
library(car)
```
## Lab work
This section of the technical appendix will cover the code produced as part of lab 6, which was used to guide the starting point for the individual project work in general in terms of datasets.


```{r lab_functions}
hadley_format <- function(data){
  for (i in 1:length(colnames(data))) {
  colnames(data)[i] = tolower(colnames(data)[i])
  }
  data
}
```


```{r lab, warning = FALSE}
#Setting up the file path and loading in the .csv files
file_loc <- "C:/Users/Stephen/Desktop/University Work/Year 3 uni/St344/"
monthYears <- paste0(month.abb[c(3:12,1:8)], "_", c(rep(18,10),rep(19,8)))
d <- list()
for (i in 1:length(monthYears)) {
  filename <- paste0(file_loc, "Appointments_GP_Daily_Aug19/CCG_CSV_", monthYears[i], ".csv")
  d[[i]] <- import(filename, setclass = "tibble")
}

#Checking whether each file has the same number of columns
a <- TRUE
for(i in 1:length(d)){
  cols_1 <- length(d[[1]])
  if(cols_1 - length(d[[i]]) != 0){
    a <- FALSE
  }
  a
}
a

#Loading in the Coventry Data from the files
covData <- tibble()
for (i in 1:length(monthYears)) {
  covData <- rbind(covData, filter(d[[i]], CCG_NAME=="NHS Coventry and Rugby CCG"))
}

#This code removes the d variable from the environment and should speed things up
#As the d variable is over 500mb and can cause performance issues in rstudio
#remove(d)

covData <- select(covData, Appointment_Date, APPT_STATUS, HCP_TYPE, APPT_MODE, TIME_BETWEEN_BOOK_AND_APPT, CCG_CODE,
                  COUNT_OF_APPOINTMENTS) %>%
  mutate(Appointment_Date = parse_date_time(Appointment_Date, "%d-%b-%Y")) %>%
  #Changing the letters to lower case to match Hadley Wickhams style guide
  hadley_format() %>%
  #updating the appointment_date variable
  mutate(appointment_data = mdy(appointment_date))

#Looking at number of appointments by each mode
appt_by_mode <- covData %>% 
  group_by(appt_mode) %>%
  summarise(count = sum(count_of_appointments))

#Produce a plot line in the lab to ensure my code thus far is correct
covData %>% 
  group_by(appointment_date) %>%
  summarise(count = sum(count_of_appointments)) %>%
  ggplot() +
  geom_point(aes(x = appointment_date, y = count,
                 colour = wday(appointment_date, label = TRUE, abbr = TRUE))) +
  labs(x = "Date", y = "Number of Combined Hospital Appointments",
       title = "Combined Hospital Visits by Date",
       colour = "Day of Week") +
  theme_bw() +
  #Can customise colours in due course if required inside next line
  scale_colour_discrete()

#importing next dataset which involves which patients are registered at each practice
app_gp_coverage <- import(paste0(file_loc,
                                 "Appointments_GP_Daily_Aug19/APPOINTMENTS_GP_COVERAGE.csv"),
                          setclass = "tibble") %>%
  mutate(Appointment_Month = parse_date_time(Appointment_Month, "%d-%b-%Y")) %>%
  hadley_format()

##combining the two datasets for future use
#Filtered to only contain extra information where necessary
data <- left_join(covData, app_gp_coverage, by = c("ccg_code" = "commissioner_organisation_code"))
data <- inner_join(covData, app_gp_coverage, by = c("ccg_code" = "commissioner_organisation_code")) %>% 
  filter(year(appointment_date) == year(appointment_month) &
           month(appointment_date) == month(appointment_month))
```

## Demand on Coventry GPs
 INSERT R Markdown intro:
Since there is no data for number of GPs per practice, there is insufficient data to work out an appointment/GP number and hence demand will be modelled by overall appointments in the Cov Area. If this data had been available, then the average time of each visit type and number could have been averaged out over the GPs to find a demand average for GPs but sadly this wasn't available. Appointments which were not attended are still used for the count as the GP still had to be available to answer the person. The counts have been summed by week to take account of differences between days and too try and *absorb* some of that variability.
There has also been no subsetting based on hcp_type -> might need to add that in potentially.

Want really to compare results for which we have two measurements for in almost a paired t-test but only 18 months is slightly problematic...

```{r demand_functions, warning=FALSE}
#Graphing function for visual aids on the appointment counts
appt_count <- function(data, filter = "No Filter"){
  if(!filter == "No Filter"){
    #Updating data with filter
    data <- data %>%
      filter(appt_mode == filter)
    #Plot with no filter (include legend)
  output <- data %>%
    ggplot(aes(x = week_num, y = count, colour = season_median)) + 
    geom_point() +
    geom_smooth(colour = "blue") + 
    geom_smooth(method = "lm", colour = "red") +
    theme_bw() +
    labs(title = paste0("Weekly Appointment Count with: ", filter),
         x = "Week Number", y = "Weekly Appointment Count", colour = "Season") +
    scale_colour_manual("Season", values = c("darkorange1", "springgreen3","gold", "royalblue2"))
  }else{
  output <- data %>%
    ggplot(aes(x = week_num, y = count, colour = appt_mode)) + 
    geom_point() +
    theme_bw()  + 
    labs(title = paste0("Weekly Appointment Count with: ", filter),
         x = "Week Number", y = "Weekly Appointment Count", colour = "Appointment Type")
  }
  
  output
}
```
 
```{r demand_setup}
#Seasons data as per: https://www.timeanddate.com/calendar/seasons.html
seasons <- tibble(season = c("winter", "spring", "summer","autumn",
                             "winter", "spring", "summer"),
                  start_date = c(as.Date("2017-12-21"), as.Date("2018-03-20"),
                                 as.Date("2018-06-20"), as.Date("2018-09-23"),
                                 as.Date("2018-12-21"), as.Date("2019-03-20"),
                                 as.Date("2019-06-20")),
                  end_date = c(as.Date("2018-03-19"), as.Date("2018-06-19"),
                               as.Date("2018-09-22"), as.Date("2018-12-20"),
                               as.Date("2019-03-19"), as.Date("2019-06-19"),
                               as.Date("2019-09-22"))) 

#Some changes to the data, adding in season and week number to each observation 
 data <- data %>%
  #Detecting week number of appointment date
  mutate(week_num = isoweek(appointment_date)) %>%
  mutate(year = year(appointment_date)) %>%
  mutate(year = if_else(year == 2018, 0, 1)) %>%
  mutate(week_num = week_num + (52 * year)) %>%
  #Taking account of the fact that 1 actually refers to week 53
  mutate(week_num = if_else(week_num == 1, 53, week_num)) %>%
  mutate(week_num = week_num - 8) %>%
  #Changing class of appointment date variable for easier comparisons
  mutate(appointment_date = appointment_date %>% substr(1,10) %>% as.Date()) %>%
  #Adding
  mutate(season = if_else(between(appointment_date,
                                  seasons$start_date[1],seasons$end_date[1]), "winter",
                  if_else(between(appointment_date,
                                  seasons$start_date[2],seasons$end_date[2]), "spring",
                  if_else(between(appointment_date
                                  , seasons$start_date[3],seasons$end_date[3]), "summer",
                  if_else(between(appointment_date
                                  , seasons$start_date[4],seasons$end_date[4]), "autumn",
                  if_else(between(appointment_date
                                  , seasons$start_date[5],seasons$end_date[5]), "winter",
                  if_else(between(appointment_date
                                  , seasons$start_date[6],seasons$end_date[6]), "spring",
                  if_else(between(appointment_date
                                  , seasons$start_date[7],seasons$end_date[7]), "summer",
                                  "Error")))))))) 
 
 
 
# Making adjustment as some weeks may contain dates from different seasons
# Use median as 7 days in a week so only modal one will occur
temp <- data %>%
   mutate(season_num = if_else(season == "winter", 1,
                       if_else(season == "spring", 2,
                       if_else(season == "summer", 3 , 4)))) %>%
   select(week_num, appointment_date,season, season_num) %>%
   unique() %>%
   select(week_num, season,season_num) %>%
   group_by(week_num) %>%
   summarise(count = median(season_num)) %>% 
   mutate(season_median = if_else(count == 1, "winter", 
                          if_else(count == 2, "spring", 
                          if_else(count == 3, "summer", "autumn")))) %>%
  select(week_num, season_median)


data <- data %>%
  inner_join(temp)

#Saving the data before I remove the anonyamous weeks
#For use in part two of the report
part_2_data <- data

#Count summaries of the data by appointment mode and week
appt_by_mode <- data %>% 
  group_by(appt_mode, week_num, season_median) %>%
  summarise(count = sum(count_of_appointments)) %>%
  ungroup()

#General plot for understanding
appt_by_mode %>%
  appt_count()

#Looking into the two low results for Face-to-Face
appt_by_mode %>% 
  filter(count < 20000 & appt_mode == "Face-to-Face")

#Returns week 9 (first week and not a full one, only 4 results) and 52 (over Christmas so less visits)
data %>%
  filter(week_num %in% c(9, 52)) %>%
  select(appointment_date) %>%
  unique()

#No appointments on 30th December - CHECK
data %>% 
  filter(str_detect(appointment_date, '12-30'))

##Therefore we remove these two weeks (9 and 52) as they seem to be anomalous in comparison to others.
data <- data %>%
  filter(!week_num %in% c(9, 52))
```

```{r demand_eda_graphs}
#Graphically exploring the different visit types and 
appt_by_mode <- data %>% 
  group_by(appt_mode, week_num, season_median) %>%
  summarise(count = sum(count_of_appointments)) %>%
  ungroup()

appt_by_mode %>%
  appt_count()

appt_by_mode %>%
  appt_count("Face-to-Face")

appt_by_mode %>%
  appt_count("Home Visit")

appt_by_mode %>%
  appt_count("Telephone")
#For the purposes of the question "Unknown" will not be considered
# appt_by_mode %>%
#   appt_count("Unknown")

#Joining the data appt_by_mode data to original data
data <- data %>%
  inner_join(appt_by_mode)
```

## Fitting a poison model to the data as in the lab
```{r}
poisson_data <- appt_by_mode %>%
  #mutate(log_registered_patients = log(`patients registered at included practices`)) %>%
  select(appointment_date, count, weekday, log_registered_patients) %>%
  unique()

my_model <- glm(count ~ 0 + week_num + season_median , family = "poisson", data = appt_by_mode) #+ offset(log_registered_patients)
```


## Statistical significance tests
Looking at the graph generated above, makes sense that season would be a factor to the count. So will be looking into that in the following code. Use Anova test if variance assumption holds - otherwise use the kruskal wallace test. Where the assumption is that all the groups have the same variance.
```{r anova_seasons}
checking_seasons <- function(data, appt_type){
  #Variance assumption (checking they are the same)
  season_check <- data %>% filter(appt_mode == appt_type) %>%
    select(appt_mode, season_median, count) %>% 
    unique()
  #if < 0.05 use anova otherwise use kruskal wallace
  var_value <- leveneTest(count ~ season_median, data = season_check)[1,3]
  if(var_value < 0.05){
    test <- "Kruskal"
    #Kruskal Wallace test
    p_value <- kruskal.test(count ~ season_median, data = season_check)$p.value
     
  }else{
    test <- "Anova"
    #Running the anova  test
  anova1 <- aov(count ~ season_median, data = season_check)
  p_value <- summary(anova1)[[1]][1,5]
  }
tibble(appt_type = appt_type, test = test, p_value = p_value)
}

checking_seasons(data, "Face-to-Face") %>%
  bind_rows(checking_seasons(data, "Telephone")) %>%
  bind_rows(checking_seasons(data, "Home Visit"))

```
Splitting the data by season for compared t-test would give an option to test if the seasons themselves have increased between the two years. Obviously the cold weather will lead to more illnesses.
The following function uses a paired t-test to calculate whether the differences are statistically significant or not. It will either use a Welch's test if the variances are not equal, a students t-test if they are or if the data is not normal then it will perform a Wilcoxon Sign Ranked Test on the data. Autumn data cannot be considered as only 1 year of autumn data.


```{r}
#Extracting the data to be used in these tests
stat_data <- data %>%
  #Give a 0 or 1 as a year column for paired t-testing.
  mutate(year == year(appointment_date)) %>%
  select(appt_mode, week_num, season_median, year, count) %>% 
  unique()
  
stat_test <- function(data, type, season){
#Updating data
temp_data <- stat_data %>%
  filter(season_median == season) %>%
  filter(appt_mode == type)  
  
#Test the normality assumption of the data
normal <- TRUE
if(shapiro.test(temp_data$count)$p.value < 0.05){
  normal <- FALSE
}

#Checking validity of equal variance
year_1 <- temp_data %>%
  filter(year == 0) %>%
  select(count) %>%
  as_vector()
year_2 <- temp_data %>%
  filter(year == 1) %>%
  select(count) %>%
  as_vector()

#Setting seed to ensure reproducibility
set.seed(666)
min_length <- min(length(year_1), length(year_2))

#Defining data in another format for latter parts
test_data <- tibble(year_1 = year_1 %>% sample(min_length),
                    year_2 = year_2 %>% sample(min_length))

#t-test section
if(normal){
  
#Sampling for F-test if numbers are different
#Replace = FALSE by default.

variance <- TRUE
#Compares the p.value from F-test and changes the variance check accordingly
if(tidy(var.test(test_data$year_1,
                   test_data$year_2)) %>%
  select(p.value) %>% as_vector() < 0.05){
 #Varaince test significant -> variances NOT Equal 
  variance <- FALSE
  }
if(variance == FALSE){
  test <- "Welch"
  #Run Welch test where variance assumption NOT assumed
  t_test <- tidy(t.test((test_data$year_2 - test_data$year_1), mu = 0,
                        alternative = "greater")) %>%
  select(p.value) %>% as_vector()
  if(t_test < 0.05){
    return("H1 True")
  }else{
    return("H0 True")
}
}else{
  test <- "Students"
  #Run students were the variance is assumed
  p_value <- tidy(t.test((test_data$year_2 - test_data$year_1), mu = 0,
                        var.equal=TRUE ,alternative = "greater")) %>%
  select(p.value)  %>% as_vector()

}

}else{
  test <- "Wilcoxon"
  #Run the wilcoxon sign ranked test if normality assumption failed
  p_value <- tidy(wilcox.test(test_data$year_2 , test_data$year_1,
                              paired = TRUE, alternative = "greater")) %>%
  select(p.value) %>% as_vector()
}
  #Conclusion
  if(t_test < 0.05){
    outcome <- "H1 True"
  }else{
    outcome <- "H0 True"
}

output <- tibble(appt_mode = type, season = season, test = test,
                 p_value = p_value, conclusion = outcome)
output



}

#Now run the paired T-Test on the data
stat_table <- stat_test(stat_data, "Telephone", "summer") %>%
  rbind(stat_test(stat_data, "Telephone", "winter")) %>%
  rbind(stat_test(stat_data, "Telephone", "spring")) %>%
  rbind(stat_test(stat_data, "Face-to-Face", "summer")) %>%
  rbind(stat_test(stat_data, "Face-to-Face", "winter")) %>%
  rbind(stat_test(stat_data, "Face-to-Face", "spring")) %>%
  rbind(stat_test(stat_data, "Home Visit", "spring")) %>%
  rbind(stat_test(stat_data, "Home Visit", "winter")) %>%
  rbind(stat_test(stat_data, "Home Visit", "spring"))
stat_table
```
The outcome from this table demonstrates that there is a statistically significant increase in the number of appointments between the 2018 and 2019 dates for all appointment types and seasons considered.


# Second assignment Questions


```{r}
data <- part_2_data

#Checking unique types of booking times
data %>% 
  select(time_between_book_and_appt) %>%
  unique()
#Checking unique types of appt_status
data %>%
  select(appt_status) %>% 
  unique()

#Discard all data with Unknown / Data Issue and
#Appointment status as Uknown or Appt Status Not Provided
#As only interested in attended/Not attended appointments

data <- data %>%
  filter(!time_between_book_and_appt == "Unknown / Data Issue") %>%
  filter(appt_status %in% c("Attended", "DNA")) %>%
  #Only looking at the GP appointments not attended
  filter(hcp_type == "GP")


data %>%
  group_by(appt_status, time_between_book_and_appt) %>%
  summarise(c = mean(count))

total_count <- data %>%
  select(appointment_date, time_between_book_and_appt, count_of_appointments) %>%
  group_by(appointment_date, time_between_book_and_appt) %>%
  summarise(total_count = sum(count_of_appointments)) %>%
  ungroup()

appt_count <- data %>%
  select(appointment_date, time_between_book_and_appt, appt_status, count_of_appointments) %>%
  group_by(appointment_date, time_between_book_and_appt, appt_status) %>%
  summarise(count = sum(count_of_appointments)) %>%
  ungroup() %>%
  inner_join(total_count) %>%
  mutate(percentage_attendence = 100 *(count/total_count))

#DNA = 100 - Attended so no need to store both
anova_data <- appt_count %>%
  filter(appt_status == "DNA")

time_anova <- aov(percentage_attendence ~ time_between_book_and_appt, data = anova_data)
summary(time_anova)  
TukeyHSD(time_anova)

```
